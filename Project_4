---
title: "Project 4"
author: 
  - Shana Green
  - Mark Gonsalves 
  - Dominika Markowska-Desvallons
  - Orli Khaimova 
  - John Mazon
date: "11/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Loading Libraries

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(tm)
library(magrittr)
library(data.table)
library(e1071)
```


### Loading the Data

```{r}
spam_folder <- 'C:/Users/Home/Desktop/MSDS/DATA 607/Project 4/spam/'
ham_folder <- 'C:/Users/Home/Desktop/MSDS/DATA 607/Project 4/easy_ham_2/'

length(list.files(path = spam_folder))

spam_files <- list.files(path = spam_folder, full.names = TRUE)
ham_files <- list.files(path = ham_folder, full.names = TRUE)

spam <- list.files(path = spam_folder) %>%
  as.data.frame() %>%
  set_colnames("file") %>%
  mutate(text = lapply(spam_files, read_lines)) %>%
  unnest(c(text)) %>%
  mutate(class = "spam",
         spam = 1) %>%
  group_by(file) %>%
  mutate(text = paste(text, collapse = " ")) %>%
  ungroup() %>%
  distinct()
            
ham <- list.files(path = ham_folder) %>%
  as.data.frame() %>%
  set_colnames("file") %>%
  mutate(text = lapply(ham_files, read_lines)) %>%
  unnest(c(text)) %>%
  mutate(class = "ham",
         spam = 0) %>%
  group_by(file) %>%
  mutate(text = paste(text, collapse = " ")) %>%
  ungroup() %>%
  distinct()

#ham <- read_lines('C:/Users/Home/Desktop/MSDS/DATA 607/Project 4/20030228_easy_ham_2.tar.bz2', 
#                  skip_empty_rows = TRUE, n_max = 10000) %>%
#  as.data.frame() %>%
#  set_colnames("text") %>%
#  mutate(class = "ham")

#spam <- read_lines('C:/Users/Home/Desktop/MSDS/DATA 607/Project 4/20030228_spam.tar.bz2', 
#                  skip_empty_rows = TRUE, n_max = 10000) %>%
#  as.data.frame() %>%
#  set_colnames("text") %>%
#  mutate(class = "spam")

```

### Tidying Data / Creating Corpus

```{r, warning = FALSE}
ham_spam <- rbind(ham, spam) %>%
  select(class, spam, file, text)

ham_spam$text <- ham_spam$text %>%
  str_replace(.,"[\\r\\n\\t]+", "")

replacePunctuation <- content_transformer(function(x) {return (gsub("[[:punct:]]", " ", x))})

#NewWords <- c("localhost", "received", "delivered", "com", "net", "org", "http", "font", "aug")
#  tm_map(removeWords, NewWords) %>%

corpus <- Corpus(VectorSource(ham_spam$text)) %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(replacePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(stripWhitespace)

dtm <- DocumentTermMatrix(corpus)

dtm <- removeSparseTerms(dtm, 1-(10/length(corpus)))

inspect(dtm)

dim(dtm)
```

### Training Dataa

First, we took the Document Term Matrix that was created before and set it as a
data frame. We also added a column which would classify each document/row as spam
or not spam. We converted the `spam` column into a factor. To split the data into
training and testing data, we decided to sample 80% as training data and 20% as
testing data. We also found the proportions of ham to spam count. We found that 
both the testing and training data had about 73% ham and 27% spam.

```{r}
email_dtm <- dtm %>%
  as.matrix() %>%
  as.data.frame() %>%
  sapply(., as.numeric) %>%
  as.data.frame() %>%
  mutate(spam = ham_spam$spam) %>%
  select(spam, everything())

#count <- data.frame(word = colnames(email_dtm),
#                    count = colSums(email_dtm)) %>%
#  filter(word != "spam")

email_dtm$spam <- as.factor(email_dtm$spam)

#Training & Test set
sample_size <- floor(0.8 * nrow(email_dtm))

set.seed(1564)
index <- sample(seq_len(nrow(email_dtm)), size = sample_size)
  
dtm_train <- email_dtm[index, ]
dtm_test <-  email_dtm[-index, ]

#Training & Test Spam Count
train_labels <- dtm_train$spam
test_labels <- dtm_test$spam

#Proportion for training & test Spam
prop.table(table(train_labels))
prop.table(table(test_labels))

```

